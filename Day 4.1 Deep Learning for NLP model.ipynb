{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdb0b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d80f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8097fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14be8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d76fc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the doc\n",
    "\n",
    "def load_doc(filename):\n",
    "    file=open(filename,'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4865c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn a doc into clean tokens\n",
    "def clean_doc(doc,vocab):\n",
    "    tokens=doc.split()\n",
    "    re_punc=re.compile('[%s]'%re.escape(string.punctuation))\n",
    "    tokens=[re_punc.sub('',w) for w in tokens]\n",
    "    tokens=[w for w in tokens if w in vocab]\n",
    "    tokens=' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b2bd1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all docs in a directory\n",
    "def process_docs(directory,vocab,is_train):\n",
    "    documents=list()\n",
    "    for filename in listdir(directory):\n",
    "        \n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        path=directory+'/'+filename\n",
    "    \n",
    "        doc=load_doc(path)\n",
    "    \n",
    "        tokens=clean_doc(doc,vocab)\n",
    "    \n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d021b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename='C:/Users/Admin/OneDrive/NLP_Mphasis_MLA_Training/vocab1.txt'\n",
    "vocab=load_doc(vocab_filename)\n",
    "vocab=set(vocab.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7adcaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "films adapted comic books plenty success whether theyre superheroes batman superman spawn geared toward kids casper arthouse crowd ghost world theres never really comic book like hell starters created alan moore eddie campbell brought medium whole new level mid series called say moore campbell thoroughly subject jack ripper would like saying michael jackson starting look little odd book graphic novel pages long includes nearly consist nothing footnotes words dont dismiss film source get past whole comic book thing might find another stumbling block hells directors albert allen hughes getting hughes brothers direct seems almost ludicrous casting carrot top well anything better direct film thats set ghetto features really violent street crime mad geniuses behind menace ii society ghetto question course whitechapel londons east end filthy place whores called unfortunates starting get little nervous mysterious psychopath carving profession surgical precision first stiff turns copper peter robbie coltrane world enough calls inspector frederick abberline johnny depp blow crack case abberline widower prophetic dreams unsuccessfully tries quell copious amounts absinthe opium upon arriving whitechapel befriends unfortunate named mary kelly heather graham say isnt proceeds investigate horribly gruesome crimes even police surgeon cant stomach dont think anyone needs jack ripper wont go particulars say moore campbell unique interesting theory identity killer reasons chooses slay comic dont bother identity ripper screenwriters terry hayes vertical limit rafael les mis rables good job keeping hidden viewers end funny watch locals blindly point finger blame jews indians englishman could never capable committing ghastly acts hells ending song simpsons days holds back electric made steve guttenberg star dont worry itll make sense see onto hells appearance certainly dark bleak enough surprising see much looks like tim burton film planet apes times seems like sleepy hollow print saw wasnt completely finished color music finalized comments marilyn manson cinematographer peter dont say word ably captures dreariness london helped make flashy killing scenes remind crazy flashbacks twin peaks even though violence film pales comparison blackandwhite comic oscar winner martin childs shakespeare love production design turns original surroundings one creepy place even acting hell solid dreamy depp turning typically strong performance deftly handling british accent holm joe goulds secret richardson dalmatians log great supporting roles big surprise graham cringed first time opened mouth imagining attempt irish accent actually wasnt half bad film however good strong violencegore sexuality language drug content\n"
     ]
    }
   ],
   "source": [
    "#load the document\n",
    "\n",
    "filename='C:/Users/Admin/OneDrive/NLP_Mphasis_MLA_Training/txt_sentoken/pos/cv000_29590.txt'\n",
    "text=load_doc(filename)\n",
    "tokens=clean_doc(text,vocab)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c115a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and clean dataset\n",
    "\n",
    "def load_clean_dataset(vocab,is_train):\n",
    "    neg=process_docs('C:/Users/Admin/OneDrive/NLP_Mphasis_MLA_Training/txt_sentoken/neg',vocab,is_train)\n",
    "    pos=process_docs('C:/Users/Admin/OneDrive/NLP_Mphasis_MLA_Training/txt_sentoken/pos',vocab,is_train)\n",
    "    docs=neg+pos\n",
    "    \n",
    "    #prepare labels\n",
    "    labels=array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba0769d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a62cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#integer encode and pad documnets\n",
    "def encode_docs(tokenizer,max_length,docs):\n",
    "    #integer encode\n",
    "    encoded=tokenizer.texts_to_sequences(docs)\n",
    "    #pad sequences\n",
    "    padded=pad_sequences(encoded,maxlen=max_length,padding='post')\n",
    "    return padded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0fd9588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "\n",
    "def model(vocab_size,max_length):\n",
    "    \n",
    "    model=Sequential()\n",
    "    \n",
    "    #input layer\n",
    "    model.add(Embedding(vocab_size,100,input_length=max_length))\n",
    "    \n",
    "    #Hidden layer\n",
    "    model.add(Conv1D(filters=32,kernel_size=8,activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    #flatten layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    #Compile the network\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "   #summarize the model\n",
    "    model.summary()\n",
    "    #plot_model(model,to_file='model.png',show_shapes=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e089080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the vocabulary\n",
    "vocab_filename='C:/Users/Admin/OneDrive/NLP_Mphasis_MLA_Training/vocab1.txt'\n",
    "vocab=load_doc(vocab_filename)\n",
    "vocab=set(vocab.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29c7caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training data\n",
    "train_docs,ytrain=load_clean_dataset(vocab,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53fc48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the tokenizer\n",
    "tokenizer=create_tokenizer(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65896f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25768"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the vocabulary size\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f8f06b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length:1317\n"
     ]
    }
   ],
   "source": [
    "#Calculate the maximum sequence length\n",
    "\n",
    "max_length=max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length:%d'%max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3214e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode data\n",
    "xtrain=encode_docs(tokenizer,max_length,train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13ffcf13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (1800, 1317))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(xtrain),xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d39dac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1317, 100)         2576800   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1310, 32)          25632     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 655, 32)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20960)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                209610    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2812053 (10.73 MB)\n",
      "Trainable params: 2812053 (10.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "57/57 - 14s - loss: 0.6893 - accuracy: 0.5417 - 14s/epoch - 240ms/step\n",
      "Epoch 2/10\n",
      "57/57 - 11s - loss: 0.5411 - accuracy: 0.7578 - 11s/epoch - 201ms/step\n",
      "Epoch 3/10\n",
      "57/57 - 11s - loss: 0.0850 - accuracy: 0.9850 - 11s/epoch - 198ms/step\n",
      "Epoch 4/10\n",
      "57/57 - 12s - loss: 0.0053 - accuracy: 0.9994 - 12s/epoch - 218ms/step\n",
      "Epoch 5/10\n",
      "57/57 - 11s - loss: 0.0021 - accuracy: 0.9994 - 11s/epoch - 200ms/step\n",
      "Epoch 6/10\n",
      "57/57 - 11s - loss: 0.0013 - accuracy: 1.0000 - 11s/epoch - 199ms/step\n",
      "Epoch 7/10\n",
      "57/57 - 11s - loss: 8.7343e-04 - accuracy: 1.0000 - 11s/epoch - 198ms/step\n",
      "Epoch 8/10\n",
      "57/57 - 11s - loss: 6.1877e-04 - accuracy: 1.0000 - 11s/epoch - 197ms/step\n",
      "Epoch 9/10\n",
      "57/57 - 11s - loss: 4.4992e-04 - accuracy: 1.0000 - 11s/epoch - 200ms/step\n",
      "Epoch 10/10\n",
      "57/57 - 11s - loss: 3.3798e-04 - accuracy: 1.0000 - 11s/epoch - 197ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "#Train the network\n",
    "model=model(vocab_size,max_length)\n",
    "model.fit(xtrain,ytrain,epochs=10,verbose=2)\n",
    "#save the model\n",
    "model.save('nlp_model_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fcb9dd",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2570397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  25768\n",
      "Maximum length:1317\n",
      "train Accuracy:\t 1.0\n",
      "\n",
      "Test Accuracy:\t 0.8550000190734863\n"
     ]
    }
   ],
   "source": [
    "train_docs,ytrain=load_clean_dataset(vocab,True)\n",
    "test_docs,ytest=load_clean_dataset(vocab,False)\n",
    "tokenizer=create_tokenizer(train_docs)\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print('Vocabulary size: ',vocab_size)\n",
    "#Calculate the maximum sequence length\n",
    "\n",
    "max_length=max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length:%d'%max_length)\n",
    "#encode data\n",
    "xtrain=encode_docs(tokenizer,max_length,train_docs)\n",
    "xtest=encode_docs(tokenizer,max_length,test_docs)\n",
    "\n",
    "#load the model\n",
    "_,acc=model.evaluate(xtrain,ytrain,verbose=0)\n",
    "print('train Accuracy:\\t',acc)\n",
    "print()\n",
    "_,acc=model.evaluate(xtest,ytest,verbose=0)\n",
    "print('Test Accuracy:\\t',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e4895",
   "metadata": {},
   "source": [
    "# classify a review as negative or positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6407a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review,vocab,tokenizer,max_length,model):\n",
    "    line=clean_doc(review,vocab)\n",
    "    padded=encode_docs(tokenizer,max_length,[line])\n",
    "    \n",
    "    ypred=model.predict(padded,verbose=0)\n",
    "    percent_pos=ypred[0,0]\n",
    "    if round(percent_pos)==0:\n",
    "        return (1-percent_pos),'Negative'\n",
    "    return percent_pos,'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85401059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.729116290807724, 'Negative')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='Everyone will enjoy the flim.I love it,recommecnded'\n",
    "predict_sentiment(text,vocab,tokenizer,max_length=max_length,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c96f117a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7327710092067719, 'Negative')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='not good'\n",
    "predict_sentiment(text,vocab,tokenizer,max_length=max_length,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6e02da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
